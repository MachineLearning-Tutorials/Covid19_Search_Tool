{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CORD19 - NLP Challenge",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7be1d3d15f5b4e9d9ee2b4f9223da84f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cccc4bd597f34b049df91a08b9669379",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_15c23c2c03104af2bd42a5dfc02d8686",
              "IPY_MODEL_ca3c430cd1bc40019f3865dd45692bbe"
            ]
          }
        },
        "cccc4bd597f34b049df91a08b9669379": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "15c23c2c03104af2bd42a5dfc02d8686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "state": {
            "_view_name": "TextView",
            "style": "IPY_MODEL_7286b8badb294af9a397db67ecad89c8",
            "_dom_classes": [],
            "description": "SearchTerms",
            "_model_name": "TextModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "ACE spike",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77a56e3eb5f045d393e7a468d717c994"
          }
        },
        "ca3c430cd1bc40019f3865dd45692bbe": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "state": {
            "_view_name": "OutputView",
            "msg_id": "",
            "_dom_classes": [],
            "_model_name": "OutputModel",
            "outputs": [
              {
                "output_type": "display_data",
                "metadata": {
                  "tags": []
                },
                "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>doi</th>\n      <th>authors</th>\n      <th>journal</th>\n      <th>publish_time</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>36th International Symposium on Intensive Care...</td>\n      <td>P001 - Sepsis impairs the capillary response w...</td>\n      <td>10.1186/s13054-016-1208-6</td>\n      <td>Bateman, R. M.; Sharpe, M. D.; Jagger, J. E.; ...</td>\n      <td>Crit Care</td>\n      <td>2016 Apr 20</td>\n      <td>48.897161</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>XXIV World Allergy Congress 2015: Seoul, Korea...</td>\n      <td>A1 Pirfenidone inhibits TGF-b1-induced extrace...</td>\n      <td>10.1186/s40413-016-0096-1</td>\n      <td>Lee, Heung-Man; Park, Il-Ho; Shin, Jae-Min; Yo...</td>\n      <td>World Allergy Organ J</td>\n      <td>2016 Apr 19</td>\n      <td>48.850667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Both ERK1 and ERK2 Are Required for Enteroviru...</td>\n      <td>It has been demonstrated that MEK1, one of the...</td>\n      <td>10.3390/v7031344</td>\n      <td>Zhu, Meng; Duan, Hao; Gao, Meng; Zhang, Hao; P...</td>\n      <td>Viruses</td>\n      <td>2015 Mar 20</td>\n      <td>48.841823</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Activation of TORC1 Transcriptional Coactivato...</td>\n      <td>CREB is a prototypic bZIP transcription factor...</td>\n      <td>10.1091/mbc.E08-04-0369</td>\n      <td>Siu, Yeung-Tung; Ching, Yick-Pang; Jin, Dong-Yan</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>48.837021</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Decoying the cap- mRNA degradation system by a...</td>\n      <td>The major coat protein of the L-A double-stran...</td>\n      <td>NaN</td>\n      <td>Masison, D C; Blanc, A; Ribas, J C; Carroll, K...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>48.809539</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>Phosphatidylinositol 4-Kinase IIIβ Is Required...</td>\n      <td>Phosphatidylinositol kinases (PI kinases) play...</td>\n      <td>10.1074/jbc.M111.312561</td>\n      <td>Yang, Ning; Ma, Ping; Lang, Jianshe; Zhang, Ya...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>48.482152</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>Novel genotype of infectious bronchitis virus ...</td>\n      <td>Abstract Recombination events are known to con...</td>\n      <td>10.1016/j.vetmic.2019.01.020</td>\n      <td>Ma, Tianxin; Xu, Liwen; Ren, Mengting; Shen, J...</td>\n      <td>Veterinary Microbiology</td>\n      <td>2019-03-31</td>\n      <td>48.482058</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>IκB kinase ɛ (IKKɛ): A therapeutic target in i...</td>\n      <td>Abstract The innate immune system forms our fi...</td>\n      <td>10.1016/j.bcp.2013.01.007</td>\n      <td>Verhelst, Kelly; Verstrepen, Lynn; Carpentier,...</td>\n      <td>Biochemical Pharmacology</td>\n      <td>2013-04-01</td>\n      <td>48.481654</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>An educational programme for nursing college s...</td>\n      <td>BACKGROUND: The Middle Eastern Respiratory Syn...</td>\n      <td>10.1186/s12912-015-0065-y</td>\n      <td>Stirling, Bridget V; Harmston, Jennie; Alsobay...</td>\n      <td>BMC Nurs</td>\n      <td>2015 Apr 16</td>\n      <td>48.481506</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>Chapter 19 Species Inquirendae in the Carnivora</td>\n      <td>Abstract There are at least 483 reports of car...</td>\n      <td>10.1016/B978-0-12-811349-3.00019-0</td>\n      <td>Duszynski, Donald W.; Kvičerová, Jana; Seville...</td>\n      <td>The Biology and Identification of the Coccidia...</td>\n      <td>2018-12-31</td>\n      <td>48.481080</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 7 columns</p>\n</div>",
                "text/plain": "<__main__.SearchResults at 0x7ff614db45c0>"
              }
            ],
            "_view_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_view_count": null,
            "_view_module_version": "1.0.0",
            "layout": "IPY_MODEL_6b65aa7979be4d45a129689a4ae65814",
            "_model_module": "@jupyter-widgets/output"
          }
        },
        "7286b8badb294af9a397db67ecad89c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77a56e3eb5f045d393e7a468d717c994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b8a3e8293e6f4320bba1daba3be7be3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b4efed94a24a439a8694f3f0a488593e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f1b50affced24423878211465d7a7a87",
              "IPY_MODEL_69f781068f1e4fcfb8cc5a71f8486e63"
            ]
          }
        },
        "b4efed94a24a439a8694f3f0a488593e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1b50affced24423878211465d7a7a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "state": {
            "_options_labels": [
              "What is known about transmission, incubation, and environmental stability?",
              "What do we know about COVID-19 risk factors?",
              "What do we know about virus genetics, origin, and evolution?",
              "What has been published about ethical and social science considerations",
              "What do we know about diagnostics and surveillance?",
              "What has been published about medical care?",
              "What do we know about vaccines and therapeutics?"
            ],
            "_view_name": "DropdownView",
            "style": "IPY_MODEL_5a4ffac021a54cbd9ed5928d417a86ca",
            "_dom_classes": [],
            "description": "Task",
            "_model_name": "DropdownModel",
            "index": 0,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f812bb34bc50426e9e116705969fde6f"
          }
        },
        "69f781068f1e4fcfb8cc5a71f8486e63": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "state": {
            "_view_name": "OutputView",
            "msg_id": "",
            "_dom_classes": [],
            "_model_name": "OutputModel",
            "outputs": [
              {
                "output_type": "stream",
                "metadata": {
                  "tags": []
                },
                "text": "What is known about transmission, incubation, and environmental stability?\n",
                "stream": "stdout"
              },
              {
                "output_type": "display_data",
                "metadata": {
                  "tags": []
                },
                "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>doi</th>\n      <th>authors</th>\n      <th>journal</th>\n      <th>publish_time</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Revisión sobre las infecciones no bacterianas ...</td>\n      <td>Resumen Aunque las bacterias son los principal...</td>\n      <td>10.1016/j.arbres.2015.02.015</td>\n      <td>Galván, José María; Rajas, Olga; Aspa, Javier</td>\n      <td>Archivos de Bronconeumología</td>\n      <td>2015-11-30</td>\n      <td>256.904116</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The 12th Edition of the Scientific Days of the...</td>\n      <td>A1 The outcome of patients with recurrent vers...</td>\n      <td>10.1186/s12879-016-1877-4</td>\n      <td>Niculae, Cristian-Mihail; Manea, Eliza; Jipa, ...</td>\n      <td>BMC Infect Dis</td>\n      <td>2016 Nov 1</td>\n      <td>256.888407</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Influence of different glycoproteins and of th...</td>\n      <td>Host plasma membrane protein SERINC5 is incorp...</td>\n      <td>10.1101/780577</td>\n      <td>Diehl, W. E.; Guney, M. H.; Kyawe, P. P.; Whit...</td>\n      <td>NaN</td>\n      <td>2019-09-24</td>\n      <td>256.872104</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Coronavirus immunogens</td>\n      <td>Abstract Coronaviruses (CV) infect a variety o...</td>\n      <td>10.1016/0378-1135(93)90030-B</td>\n      <td>Saif, Linda J.</td>\n      <td>Veterinary Microbiology</td>\n      <td>1993-11-30</td>\n      <td>256.812236</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sources et devenir des micro-organismes pathog...</td>\n      <td>Résumé Un grand nombre d’infections humaines d...</td>\n      <td>10.1016/S1773-035X(14)72362-7</td>\n      <td>Baudart, Julia; Paniel, Nathalie</td>\n      <td>Revue Francophone des Laboratoires</td>\n      <td>2014-02-28</td>\n      <td>256.795673</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>Apport de la biologie moléculaire dans l’ident...</td>\n      <td>Résumé Les débuts de la virologie furent longs...</td>\n      <td>10.1016/S1773-035X(09)70307-7</td>\n      <td>Borde, Chloé; Maréchal, Vincent; Barnay-Verdie...</td>\n      <td>Revue Francophone des Laboratoires</td>\n      <td>2009-12-31</td>\n      <td>256.198831</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>Progress towards a higher taxonomy of viruses</td>\n      <td>Summary The current consensus view is that a h...</td>\n      <td>10.1016/S0923-2516(06)80059-2</td>\n      <td>Ward, C.W.</td>\n      <td>Research in Virology</td>\n      <td>1993-12-31</td>\n      <td>256.198763</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>Identification and Characterization of a Ribos...</td>\n      <td>The order Nidovirales currently comprises four...</td>\n      <td>10.1128/JVI.00658-16</td>\n      <td>Zeng, Cong; Wu, Andong; Wang, Yi; Xu, Shan; Ta...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>256.198509</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>In vitro antimicrobial activities of animal-us...</td>\n      <td>BACKGROUND: The quinoxaline 1,4-di-N-oxides (Q...</td>\n      <td>10.1186/s12917-016-0812-7</td>\n      <td>Zhao, Yan; Cheng, Guyue; Hao, Haihong; Pan, Yu...</td>\n      <td>BMC Vet Res</td>\n      <td>2016 Sep 6</td>\n      <td>256.198353</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>Indoor air pollution and exposure assessment o...</td>\n      <td>Abstract Indoor air pollution is one of the hu...</td>\n      <td>10.1016/j.envint.2018.09.043</td>\n      <td>Amoatey, Patrick; Omidvarborna, Hamid; Baawain...</td>\n      <td>Environment International</td>\n      <td>2018-12-31</td>\n      <td>256.198131</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 7 columns</p>\n</div>",
                "text/plain": "<__main__.SearchResults at 0x7ff6166ddac8>"
              }
            ],
            "_view_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_view_count": null,
            "_view_module_version": "1.0.0",
            "layout": "IPY_MODEL_0ff7ba534e6942c7a5625874cc2666a1",
            "_model_module": "@jupyter-widgets/output"
          }
        },
        "5a4ffac021a54cbd9ed5928d417a86ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f812bb34bc50426e9e116705969fde6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cepc_sQETKhK",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqI2rescS2Ne",
        "colab_type": "code",
        "outputId": "c483fe0d-b87e-4917-aae0-f7756939a357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pathlib import Path, PurePath\n",
        "import pandas as pd\n",
        "import requests\n",
        "from requests.exceptions import HTTPError, ConnectionError\n",
        "from ipywidgets import interact\n",
        "import ipywidgets as widgets\n",
        "import re\n",
        "from ipywidgets import interact\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPoA0gobj4ck",
        "colab_type": "text"
      },
      "source": [
        "### Connect to personal google drive google drive to enable data download\n",
        "Requires you to have this data available on your personal drive: https://drive.google.com/open?id=1ZVxvPnrnA8ffGoFsVxJs75QL9li6AfG7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skzxBe_NjzII",
        "colab_type": "code",
        "outputId": "b0eb7814-5ae0-4d32-de11-8c79c776bef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive # for connecting to dataset on personal google drive\n",
        "# mount personal google drive that has data uploaded (Requires verification)\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C076btW_cSYW",
        "colab_type": "text"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mMO08ivinJ-",
        "colab_type": "code",
        "outputId": "3dc6bccd-b414-4254-8124-59660f459e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# upload data and list contents\n",
        "input_dir = PurePath('/content/drive/My Drive/CORD-19-research-challenge')\n",
        "list(Path(input_dir).glob('*'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/content/drive/My Drive/CORD-19-research-challenge/biorxiv_medrxiv'),\n",
              " PosixPath('/content/drive/My Drive/CORD-19-research-challenge/comm_use_subset'),\n",
              " PosixPath('/content/drive/My Drive/CORD-19-research-challenge/noncomm_use_subset'),\n",
              " PosixPath('/content/drive/My Drive/CORD-19-research-challenge/custom_license'),\n",
              " PosixPath('/content/drive/My Drive/CORD-19-research-challenge/metadata.readme'),\n",
              " PosixPath('/content/drive/My Drive/CORD-19-research-challenge/json_schema.txt'),\n",
              " PosixPath('/content/drive/My Drive/CORD-19-research-challenge/COVID.DATA.LIC.AGMT.pdf'),\n",
              " PosixPath('/content/drive/My Drive/CORD-19-research-challenge/metadata.csv')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhqGnR0GjsxQ",
        "colab_type": "code",
        "outputId": "aa607be8-10fd-4afa-ac07-c9260cf10ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "metadata_path = input_dir / 'metadata.csv'\n",
        "metadata = pd.read_csv(metadata_path,\n",
        "                               dtype={'Microsoft Academic Paper ID': str,\n",
        "                                      'pubmed_id': str})\n",
        "\n",
        "# Set the abstract to the paper title if it is null\n",
        "metadata.abstract = metadata.abstract.fillna(metadata.title)\n",
        "print(\"Number of articles before removing duplicates: %s \" % len(metadata))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of articles before removing duplicates: 44220 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hJqOLjmTUaE",
        "colab_type": "code",
        "outputId": "04eec6a8-23fe-4621-e4cd-8ff2e9cfa75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Some papers are duplicated since they were collected from separate sources. Thanks Joerg Rings\n",
        "duplicate_paper = ~(metadata.title.isnull() | metadata.abstract.isnull() | metadata.publish_time.isnull()) & (metadata.duplicated(subset=['title', 'abstract']))\n",
        "metadata.dropna(subset=['publish_time', 'journal'])\n",
        "metadata = metadata[~duplicate_paper].reset_index(drop=True)\n",
        "print(\"Number of articles AFTER removing duplicates: %s \" % len(metadata))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of articles AFTER removing duplicates: 42713 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr-64L0GEB2l",
        "colab_type": "text"
      },
      "source": [
        "### **TODO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfGQHLQOBwCp",
        "colab_type": "code",
        "outputId": "c88cabbc-7aba-4d2a-9ff1-c73f6bf8a068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# REMOVE articles missing publish_date or journal name\n",
        "print(\"Number of articles AFTER removing missing date and journal: %s \" % len(metadata))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of articles AFTER removing missing date and journal: 42713 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3SQuCAvkjsq",
        "colab_type": "text"
      },
      "source": [
        "### Create Data Classes for the Research Dataset and Papers\n",
        "These classes make it easier to navigate through the datasources. There is a class called ResearchPapers that wraps the entire dataset an provide useful functions to navigate through it, and Paper, that make it easier to view each paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02vqttFfcYcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get(url, timeout=6):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=timeout)\n",
        "        return r.text\n",
        "    except ConnectionError:\n",
        "        print(f'Cannot connect to {url}')\n",
        "        print(f'Remember to turn Internet ON in the Kaggle notebook settings')\n",
        "    except HTTPError:\n",
        "        print('Got http error', r.status, r.text)\n",
        "\n",
        "# Convert the doi to a url\n",
        "def doi_url(d): \n",
        "    return f'http://{d}' if d.startswith('doi.org') else f'http://doi.org/{d}'\n",
        "\n",
        "\n",
        "class ResearchPapers:\n",
        "    \n",
        "    def __init__(self, metadata: pd.DataFrame):\n",
        "        self.metadata = metadata\n",
        "        \n",
        "    def __getitem__(self, item):\n",
        "        return Paper(self.metadata.iloc[item])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "    \n",
        "    def head(self, n):\n",
        "        return ResearchPapers(self.metadata.head(n).copy().reset_index(drop=True))\n",
        "    \n",
        "    def tail(self, n):\n",
        "        return ResearchPapers(self.metadata.tail(n).copy().reset_index(drop=True))\n",
        "    \n",
        "    def abstracts(self):\n",
        "        return self.metadata.abstract.dropna()\n",
        "    \n",
        "    def titles(self):\n",
        "        return self.metadata.title.dropna()\n",
        "        \n",
        "    def _repr_html_(self):\n",
        "        return self.metadata._repr_html_()\n",
        "    \n",
        "class Paper:\n",
        "    \n",
        "    '''\n",
        "    A single research paper\n",
        "    '''\n",
        "    def __init__(self, item):\n",
        "        self.paper = item.to_frame().fillna('')\n",
        "        self.paper.columns = ['Value']\n",
        "    \n",
        "    def doi(self):\n",
        "        return self.paper.loc['doi'].values[0]\n",
        "    \n",
        "    def html(self):\n",
        "        '''\n",
        "        Load the paper from doi.org and display as HTML. Requires internet to be ON\n",
        "        '''\n",
        "        if self.doi():\n",
        "            url = doi_url(self.doi()) \n",
        "            text = get(url)\n",
        "            return widgets.HTML(text)\n",
        "    \n",
        "    def text(self):\n",
        "        '''\n",
        "        Load the paper from doi.org and display as text. Requires Internet to be ON\n",
        "        '''\n",
        "        text = get(self.doi())\n",
        "        return text\n",
        "    \n",
        "    def abstract(self):\n",
        "        return self.paper.loc['abstract'].values[0]\n",
        "    \n",
        "    def title(self):\n",
        "        return self.paper.loc['title'].values[0]\n",
        "    \n",
        "    def authors(self, split=False):\n",
        "        '''\n",
        "        Get a list of authors\n",
        "        '''\n",
        "        authors = self.paper.loc['authors'].values[0]\n",
        "        if not authors:\n",
        "            return []\n",
        "        if not split:\n",
        "            return authors\n",
        "        if authors.startswith('['):\n",
        "            authors = authors.lstrip('[').rstrip(']')\n",
        "            return [a.strip().replace(\"\\'\", \"\") for a in authors.split(\"\\',\")]\n",
        "        \n",
        "        # Todo: Handle cases where author names are separated by \",\"\n",
        "        return [a.strip() for a in authors.split(';')]\n",
        "        \n",
        "    def _repr_html_(self):\n",
        "        return self.paper._repr_html_()\n",
        "    \n",
        "\n",
        "papers = ResearchPapers(metadata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuVbNpd5nPxv",
        "colab_type": "text"
      },
      "source": [
        "#### Show a Paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAQ6vb9ynO3E",
        "colab_type": "code",
        "outputId": "54bbf7de-7102-4218-ff59-c290a575ffa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "papers[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>sha</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>source_x</th>\n",
              "      <td>Elsevier</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>title</th>\n",
              "      <td>Coronaviruses in Balkan nephritis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>doi</th>\n",
              "      <td>10.1016/0002-8703(80)90355-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pmcid</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pubmed_id</th>\n",
              "      <td>6243850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>license</th>\n",
              "      <td>els-covid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abstract</th>\n",
              "      <td>Coronaviruses in Balkan nephritis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>publish_time</th>\n",
              "      <td>1980-03-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>authors</th>\n",
              "      <td>Georgescu, Leonida; Diosi, Peter; Buţiu, Ioan;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>journal</th>\n",
              "      <td>American Heart Journal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Microsoft Academic Paper ID</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WHO #Covidence</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>has_full_text</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>full_text_file</th>\n",
              "      <td>custom_license</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<__main__.Paper at 0x7ff614ddcd30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgoC-KGBnJwZ",
        "colab_type": "text"
      },
      "source": [
        "#### Pull info from a paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snv1YrO4cY_H",
        "colab_type": "code",
        "outputId": "85d669cf-4992-4deb-9e94-408942d6538a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "index=1\n",
        "paper=papers[index]\n",
        "print(\"Example paper #%s\\nTitle: %s\\nAuthors: %s \" % (index, paper.title(), paper.authors(split=True)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example paper #1\n",
            "Title: Coronaviruses in Balkan nephritis\n",
            "Authors: ['Georgescu, Leonida', 'Diosi, Peter', 'Buţiu, Ioan', 'Plavoşin, Livia', 'Herzog, Georgeta'] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLdUa4VRn_a_",
        "colab_type": "text"
      },
      "source": [
        "### Text Preprocessing\n",
        "To prepare the text for the search index we perform the following steps\n",
        "1.   Remove punctuations and special characters\n",
        "2.   Convert to lowercase\n",
        "3.   Tokenize into individual tokens (words mostly)\n",
        "4.   Remove stopwords like (and, to))\n",
        "5.   Lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcGK5UOApijH",
        "colab_type": "code",
        "outputId": "a50994cf-8f04-456e-d6c9-7f9718fbd84e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Download the stop words we plan on using\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxTjWknWprdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hardcode the data we want to use in search\n",
        "SEARCH_DISPLAY_COLUMNS = ['title', 'abstract', 'doi', 'authors', 'journal', 'publish_time']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxPK2uLelsZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_stopwords = list(set(stopwords.words('english')))\n",
        "\n",
        "def strip_characters(text):\n",
        "    t = re.sub('\\(|\\)|:|,|;|\\.|’|”|“|\\?|%|>|<', '', text)\n",
        "    t = re.sub('/', ' ', t)\n",
        "    t = t.replace(\"'\",'')\n",
        "    return t\n",
        "\n",
        "def clean(text):\n",
        "    t = text.lower()\n",
        "    t = strip_characters(t)\n",
        "    return t\n",
        "\n",
        "def tokenize(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return list(set([word for word in words \n",
        "                     if len(word) > 1\n",
        "                     and not word in english_stopwords\n",
        "                     and not (word.isnumeric() and len(word) is not 4)\n",
        "                     and (not word.isnumeric() or word.isalpha())] )\n",
        "               )\n",
        "    \n",
        "def lemmatize(word_list,lemmatizer):\n",
        "    # Init the Wordnet Lemmatizer\n",
        "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "    return lemmatized_output\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    t = clean(text)\n",
        "    tokens = tokenize(t)\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    tokens = lemmatize(tokens,lemmatizer)\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQlY0SyEoUIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SearchResults:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 data: pd.DataFrame,\n",
        "                 columns = None):\n",
        "        self.results = data\n",
        "        if columns:\n",
        "            self.results = self.results[columns]\n",
        "            \n",
        "    def __getitem__(self, item):\n",
        "        return Paper(self.results.loc[item])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.results)\n",
        "        \n",
        "    def _repr_html_(self):\n",
        "        return self.results._repr_html_()\n",
        "\n",
        "class WordTokenIndex:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 corpus: pd.DataFrame, \n",
        "                 columns=SEARCH_DISPLAY_COLUMNS):\n",
        "        self.corpus = corpus\n",
        "        raw_search_str = self.corpus.abstract.fillna('') + ' ' + self.corpus.title.fillna('')\n",
        "        self.index = raw_search_str.apply(preprocess).to_frame()\n",
        "        self.index.columns = ['terms']\n",
        "        self.index.index = self.corpus.index\n",
        "        self.columns = columns\n",
        "\n",
        "    \n",
        "    def search(self, search_string):\n",
        "        search_terms = preprocess(search_string)\n",
        "        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n",
        "        results = self.corpus[result_index].copy().reset_index().rename(columns={'index':'paper'})\n",
        "        return SearchResults(results, self.columns + ['paper'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDnsymvcnak9",
        "colab_type": "text"
      },
      "source": [
        "### Creating a search index¶ - Using a RankBM25 Search Index\n",
        "We will create a simple search index that will just match search tokens in a document. First we tokenize the abstract and store it in a dataframe. Then we just match search terms against it.\n",
        "\n",
        "RankBM25 is a python library that implements algorithms for a simple search index. https://pypi.org/project/rank-bm25/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm9PF76UlsN6",
        "colab_type": "code",
        "outputId": "dfbaa4b5-c4ed-456e-b92a-1900008d1976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install rank_bm25\n",
        "# Create a prebaked search engine with existing package: https://pypi.org/project/rank-bm25/\n",
        "from rank_bm25 import BM25Okapi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.6/dist-packages (0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank_bm25) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa0BSZXFo9OB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RankBM25Index(WordTokenIndex):\n",
        "    \n",
        "    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n",
        "        super().__init__(corpus, columns)\n",
        "        self.bm25 = BM25Okapi(self.index.terms.tolist())\n",
        "        \n",
        "    def search(self, search_string, n=4):\n",
        "        search_terms = preprocess(search_string)\n",
        "        doc_scores = self.bm25.get_scores(search_terms)\n",
        "        ind = np.argsort(doc_scores)[::-1][:n]\n",
        "        results = self.corpus.iloc[ind][self.columns]\n",
        "        results['Score'] = doc_scores[ind]\n",
        "        results = results[results.Score > 0]\n",
        "        return SearchResults(results.reset_index(), self.columns + ['Score'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQqNtiMb-3rc",
        "colab_type": "text"
      },
      "source": [
        "### Create the index (This takes several minutes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUsxYcrUobbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bm25_index = RankBM25Index(metadata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOwEu4pgGb7n",
        "colab_type": "text"
      },
      "source": [
        "### Search by date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4RubsMNpGK2",
        "colab_type": "code",
        "outputId": "c5307f8f-d031-4503-f480-ec92839ef172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# example output\n",
        "query='curise ship'\n",
        "n=50\n",
        "results = bm25_index.search(query,n)\n",
        "results.results.sort_values(by=['publish_time'], ascending=False).head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>doi</th>\n",
              "      <th>authors</th>\n",
              "      <th>journal</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Inhibition of SARS-CoV-2 infection (previously...</td>\n",
              "      <td>AbstractThe recent outbreak of coronavirus dis...</td>\n",
              "      <td>10.1101/2020.03.09.983247</td>\n",
              "      <td>Shuai Xia; Meiqin Liu; Chao Wang; Wei Xu; Qiao...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-12</td>\n",
              "      <td>59.883227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Will novel virus go pandemic or be contained?</td>\n",
              "      <td>The repatriation of 565 Japanese citizens from...</td>\n",
              "      <td>10.1126/science.367.6478.610</td>\n",
              "      <td>Kupferschmidt, Kai; Cohen, Jon</td>\n",
              "      <td>Science</td>\n",
              "      <td>2020-02-06</td>\n",
              "      <td>59.863036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Qu’apprend-t-on de nouveau des épidémies émerg...</td>\n",
              "      <td>Points essentiels L’Afrique et l’Asie du Sud-E...</td>\n",
              "      <td>10.1016/j.lpm.2019.09.036</td>\n",
              "      <td>Malvy, Denis; Gaüzère, Bernard-Alex; Migliani,...</td>\n",
              "      <td>La Presse Médicale</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>59.869735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Alpha herpesvirus egress and spread from neuro...</td>\n",
              "      <td>Alpha herpesviruses naturally infect the perip...</td>\n",
              "      <td>10.1101/729830</td>\n",
              "      <td>Ambrosini, A. E.; Deshmukh, N.; Berry, M. J.; ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-08-08</td>\n",
              "      <td>59.938204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>A Comprehensive Review of Autophagy and Its Va...</td>\n",
              "      <td>Autophagy (self-eating) is a conserved cellula...</td>\n",
              "      <td>10.3390/cells8070674</td>\n",
              "      <td>Khandia, Rekha; Dadar, Maryam; Munjal, Ashok; ...</td>\n",
              "      <td>Cells</td>\n",
              "      <td>2019 Jul 3</td>\n",
              "      <td>59.871299</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                title  ...      Score\n",
              "23  Inhibition of SARS-CoV-2 infection (previously...  ...  59.883227\n",
              "44      Will novel virus go pandemic or be contained?  ...  59.863036\n",
              "33  Qu’apprend-t-on de nouveau des épidémies émerg...  ...  59.869735\n",
              "10  Alpha herpesvirus egress and spread from neuro...  ...  59.938204\n",
              "31  A Comprehensive Review of Autophagy and Its Va...  ...  59.871299\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulCpY3ooEr4u",
        "colab_type": "code",
        "outputId": "054d7aa7-7f6c-4ab7-afea-e32d1e6deade",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# example output\n",
        "query='ACE spike'\n",
        "n=50\n",
        "results = bm25_index.search(query,n)\n",
        "results.results.sort_values(by=['publish_time'], ascending=False).head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>doi</th>\n",
              "      <th>authors</th>\n",
              "      <th>journal</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Will novel virus go pandemic or be contained?</td>\n",
              "      <td>The repatriation of 565 Japanese citizens from...</td>\n",
              "      <td>10.1126/science.367.6478.610</td>\n",
              "      <td>Kupferschmidt, Kai; Cohen, Jon</td>\n",
              "      <td>Science</td>\n",
              "      <td>2020-02-06</td>\n",
              "      <td>48.658186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>EGR1 upregulation following Venezuelan equine ...</td>\n",
              "      <td>Abstract Venezuelan equine encephalitis virus ...</td>\n",
              "      <td>10.1016/j.virol.2019.10.016</td>\n",
              "      <td>Dahal, Bibha; Lin, Shih-Chao; Carey, Brian D.;...</td>\n",
              "      <td>Virology</td>\n",
              "      <td>2020-01-02</td>\n",
              "      <td>48.803227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Identification of a Novel Linear B-Cell Epitop...</td>\n",
              "      <td>Porcine deltacoronavirus (PDCoV), first identi...</td>\n",
              "      <td>10.3390/ijms21020648</td>\n",
              "      <td>Fu, Jiayu; Chen, Rui; Hu, Jingfei; Qu, Huan; Z...</td>\n",
              "      <td>Int J Mol Sci</td>\n",
              "      <td>2020 Jan 19</td>\n",
              "      <td>48.691775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Clinician perceptions of respiratory infection...</td>\n",
              "      <td>Abstract Outbreaks of emerging and re-emerging...</td>\n",
              "      <td>10.1016/j.idh.2019.01.003</td>\n",
              "      <td>Barratt, Ruth; Shaban, Ramon Z.; Gilbert, Gwen...</td>\n",
              "      <td>Infection, Disease &amp; Health</td>\n",
              "      <td>2019-08-31</td>\n",
              "      <td>48.759396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>ULK1/2 Restricts the Formation of Inducible SI...</td>\n",
              "      <td>Membraneless organelles (MLOs) are liquid-like...</td>\n",
              "      <td>10.1016/j.isci.2019.08.001</td>\n",
              "      <td>Saul, Vera Vivian; Seibert, Markus; Krüger, Ma...</td>\n",
              "      <td>iScience</td>\n",
              "      <td>2019 Aug 6</td>\n",
              "      <td>48.730643</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                title  ...      Score\n",
              "39      Will novel virus go pandemic or be contained?  ...  48.658186\n",
              "6   EGR1 upregulation following Venezuelan equine ...  ...  48.803227\n",
              "24  Identification of a Novel Linear B-Cell Epitop...  ...  48.691775\n",
              "10  Clinician perceptions of respiratory infection...  ...  48.759396\n",
              "17  ULK1/2 Restricts the Formation of Inducible SI...  ...  48.730643\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQLY3TWUAnG3",
        "colab_type": "text"
      },
      "source": [
        "### Creating an Autocomplete Search bar with ranking by score\n",
        "Here we provide a search bar with autocomplete. This uses IPywidgets interactive rendering of a TextBox."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beMz_ebQAr74",
        "colab_type": "code",
        "outputId": "d1b15cb2-5723-4b61-81bc-07cc0ee1ab52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622,
          "referenced_widgets": [
            "7be1d3d15f5b4e9d9ee2b4f9223da84f",
            "cccc4bd597f34b049df91a08b9669379",
            "15c23c2c03104af2bd42a5dfc02d8686",
            "ca3c430cd1bc40019f3865dd45692bbe",
            "7286b8badb294af9a397db67ecad89c8",
            "77a56e3eb5f045d393e7a468d717c994"
          ]
        }
      },
      "source": [
        "def search_papers(SearchTerms: str):\n",
        "    results_to_consider=200\n",
        "    results_to_display=10\n",
        "    # gather search results by score\n",
        "    output = bm25_index.search(SearchTerms, n=results_to_consider)\n",
        "    # sort results by recency\n",
        "    # output=search_results.results.sort_values(by=['publish_time'], ascending=False).head(results_to_display)\n",
        "    if len(output) > 0:\n",
        "        display(output) \n",
        "    return output\n",
        "\n",
        "searchbar = widgets.interactive(search_papers, SearchTerms='ACE spike')\n",
        "searchbar"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7be1d3d15f5b4e9d9ee2b4f9223da84f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "interactive(children=(Text(value='ACE spike', description='SearchTerms'), Output()), _dom_classes=('widget-int…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFTCTL0UGLlM",
        "colab_type": "text"
      },
      "source": [
        "### TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77C-cve8GMmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do search with option to restrict years available"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvkuSN65_spU",
        "colab_type": "text"
      },
      "source": [
        "### Looking at the Covid Research Tasks\n",
        "This dataset has a number of tasks. We will try to organize the papers according to the tasks\n",
        "\n",
        "What is known about transmission, incubation, and environmental stability?\n",
        "What do we know about COVID-19 risk factors?\n",
        "What do we know about virus genetics, origin, and evolution?\n",
        "What has been published about ethical and social science considerations?\n",
        "What do we know about diagnostics and surveillance?\n",
        "What has been published about medical care?\n",
        "What do we know about non-pharmaceutical interventions?\n",
        "What has been published about information sharing and inter-sectoral collaboration?\n",
        "What do we know about vaccines and therapeutics?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCAtXGvh_XAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tasks = [('What is known about transmission, incubation, and environmental stability?', \n",
        "        'transmission incubation environment coronavirus'),\n",
        "        ('What do we know about COVID-19 risk factors?', 'risk factors'),\n",
        "        ('What do we know about virus genetics, origin, and evolution?', 'genetics origin evolution'),\n",
        "        ('What has been published about ethical and social science considerations','ethics ethical social'),\n",
        "        ('What do we know about diagnostics and surveillance?','diagnose diagnostic surveillance'),\n",
        "        ('What has been published about medical care?', 'medical care'),\n",
        "        ('What do we know about vaccines and therapeutics?', 'vaccines vaccine vaccinate therapeutic therapeutics')] \n",
        "tasks = pd.DataFrame(tasks, columns=['Task', 'Keywords'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_2xnawh_z98",
        "colab_type": "text"
      },
      "source": [
        "#### Research papers for each task\n",
        "Here we add a dropdown that allows for selection of tasks and show the search results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d65a702e-aa62-4f82-a63d-fd47bd9ebac0",
        "id": "cDtB9ggPFcMq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638,
          "referenced_widgets": [
            "b8a3e8293e6f4320bba1daba3be7be3d",
            "b4efed94a24a439a8694f3f0a488593e",
            "f1b50affced24423878211465d7a7a87",
            "69f781068f1e4fcfb8cc5a71f8486e63",
            "5a4ffac021a54cbd9ed5928d417a86ca",
            "f812bb34bc50426e9e116705969fde6f"
          ]
        }
      },
      "source": [
        "def show_task(Task):\n",
        "    print(Task)\n",
        "    keywords = tasks[tasks.Task == Task].Keywords.values[0]\n",
        "    search_results = bm25_index.search(keywords, n=200)\n",
        "    return search_results\n",
        "    \n",
        "results = interact(show_task, Task = tasks.Task.tolist());"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8a3e8293e6f4320bba1daba3be7be3d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='Task', options=('What is known about transmission, incubation, and…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67PH21y__XYn",
        "colab_type": "text"
      },
      "source": [
        "# Create a BERT sentance encoding search engine \n",
        "From: https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a\n",
        "By: Denis Antyukhov\n",
        "In this experiment, we will use a pre-trained BERT model checkpoint to build a general-purpose text feature extractor.\n",
        "\n",
        "These things are sometimes referred to as Natural Language Understanding (NLU) modules, because the features they extract are relevant for a wide array of downstream NLP tasks.\n",
        "\n",
        "One use for these features is in instance-based learning, which relies on computing the similarity of the query to the training samples.\n",
        "\n",
        "We will illustrate this by building a simple Information Retrieval system using the BERT NLU module for feature extraction.\n",
        "\n",
        "**The plan for this experiment is:**\n",
        "1. getting the pre-trained BERT model checkpoint\n",
        "2. extracting a sub-graph optimized for inference\n",
        "3. creating a feature extractor with tf.Estimator\n",
        "4. exploring vector space with T-SNE and Embedding Projector\n",
        "5. implementing an Information Retrieval engine\n",
        "6. accelerating search queries with math\n",
        "7. building a covid research article recommendation system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbCgocXVJ4bY",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: getting the pre-trained model\n",
        "We start with a pre-trained english BERT-base model checkpoint.\n",
        "\n",
        "For configuring and optimizing the graph for inference we will use bert-as-a-service repository, which allows for serving BERT models for remote clients over TCP.\n",
        "\n",
        "Having a remote BERT-server is beneficial in multi-host environments. However, in this part of the experiment we will focus on creating a local (in-process) feature extractor. This is useful if one wishes to avoid additional latency and potential failure modes introduced by a client-server architecture. Now, let us download the model and install the package.\n",
        "\n",
        "Now, let us download the model and install the package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obm1_Pti_gqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip\n",
        "!pip install bert-serving-server --no-deps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGlziSVYK0gy",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: optimizing the inference graph\n",
        "Normally, to modify the model graph we would have to do some low-level TensorFlow programming. \n",
        "\n",
        "However, thanks to bert-as-a-service, we can configure the inference graph using a simple CLI interface.\n",
        "\n",
        "There are a couple of parameters in the below snippet too look out for.\n",
        "\n",
        "For each text sample, BERT-base model encoding layers output a tensor of shape **[sequence_len, encoder_dim],** with one vector per input token. To obtain a fixed representation, we need to apply some sort of pooling.\n",
        "\n",
        "**POOL_STRAT** parameter defines the pooling strategy applied to the  **POOL_LAYER** encoding layer. The default value **REDUCE_MEAN** averages the vectors for all tokens in a sequence. This strategy works best for most sentence-level tasks, when the model is not fine-tuned. Another option is NONE, in which case no pooling is applied at all. This is useful for word-level tasks such as Named Entity Recognition or POS tagging. For a detailed discussion of other options check out the Han Xiao's [blog post.](https://hanxiao.github.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/)\n",
        "\n",
        "**SEQ_LEN** affects the maximum length of sequences processed by the model. Smaller values increase the model inference speed almost linearly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkYOjgI1_ep3",
        "colab_type": "code",
        "outputId": "d40d750e-8dd9-403c-c3d4-3368f7358a85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "sesh = tf.InteractiveSession()\n",
        "\n",
        "from bert_serving.server.graph import optimize_graph\n",
        "from bert_serving.server.helper import get_args_parser\n",
        "\n",
        "# input dir\n",
        "MODEL_DIR = '/content/uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "# output dir\n",
        "GRAPH_DIR = '/content/graph/' #@param {type:\"string\"}\n",
        "# output filename\n",
        "GRAPH_OUT = 'extractor.pbtxt' #@param {type:\"string\"}\n",
        "\n",
        "POOL_STRAT = 'REDUCE_MEAN' #@param ['REDUCE_MEAN', 'REDUCE_MAX', \"NONE\"]\n",
        "POOL_LAYER = '-2' #@param {type:\"string\"}\n",
        "SEQ_LEN = '256' #@param {type:\"string\"}\n",
        "\n",
        "tf.gfile.MkDir(GRAPH_DIR)\n",
        "\n",
        "parser = get_args_parser()\n",
        "carg = parser.parse_args(args=['-model_dir', MODEL_DIR,\n",
        "                               '-graph_tmp_dir', GRAPH_DIR,\n",
        "                               '-max_seq_len', str(SEQ_LEN),\n",
        "                               '-pooling_layer', str(POOL_LAYER),\n",
        "                               '-pooling_strategy', POOL_STRAT])\n",
        "\n",
        "tmp_name, config = optimize_graph(carg)\n",
        "graph_fout = os.path.join(GRAPH_DIR, GRAPH_OUT)\n",
        "\n",
        "tf.gfile.Rename(\n",
        "    tmp_name,\n",
        "    graph_fout,\n",
        "    overwrite=True\n",
        ")\n",
        "print(\"\\nSerialized graph to {}\".format(graph_fout))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_serving/server/helper.py:186: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_serving/server/helper.py:186: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 53]:model config: /content/uncased_L-12_H-768_A-12/bert_config.json\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 56]:checkpoint: /content/uncased_L-12_H-768_A-12/bert_model.ckpt\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 60]:build graph...\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:load parameters from checkpoint...\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:136]:optimize...\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:144]:freeze...\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:149]:write graph to a tmp file: /content/graph/tmpku3r0q6a\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Serialized graph to /content/graph/extractor.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q2yDu4wLAkN",
        "colab_type": "text"
      },
      "source": [
        "Running the above snippet will put the BERT model graph and weights from  **MODEL_DIR** into a GraphDef object which will be serialized to a pbtxt file at **GRAPH_OUT**. The file will be smaller than the original model because the nodes and variables required for training will be removed. This results in a quite portable solution: for example the english base model only takes 389 MB after exporting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eARzl4sCLI7d",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: creating a feature extractor\n",
        "Now, we will use the serialized graph to build a feature extractor using the tf.Estimator API. We will need to define two things: **input_fn** and **model_fn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFrwq2AZLCkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.python.estimator.estimator import Estimator\n",
        "from tensorflow.python.estimator.run_config import RunConfig\n",
        "from tensorflow.python.estimator.model_fn import EstimatorSpec\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "from bert_serving.server.bert.tokenization import FullTokenizer\n",
        "from bert_serving.server.bert.extract_features import convert_lst_to_features\n",
        "\n",
        "\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "log.handlers = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zM-IaRxLNmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GRAPH_PATH = \"/content/graph/extractor.pbtxt\" #@param {type:\"string\"}\n",
        "VOCAB_PATH = \"/content/uncased_L-12_H-768_A-12/vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "SEQ_LEN = 256 #@param {type:\"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z885uqROLSN6",
        "colab_type": "text"
      },
      "source": [
        "**input_fn** manages getting the data into the model. That includes executing the whole text preprocessing pipeline and preparing a feed_dict for BERT. \n",
        "\n",
        "First, each text sample is converted into a tf.Example instance containing the necessary features listed in **INPUT_NAMES**. The bert_tokenizer object contains  the WordPiece vocabulary and performs the text preprocessing. After that the examples are re-grouped by feature name in a **feed_dict**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-SMkDVYLP1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_NAMES = ['input_ids', 'input_mask', 'input_type_ids']\n",
        "bert_tokenizer = FullTokenizer(VOCAB_PATH)\n",
        "\n",
        "def build_feed_dict(texts):\n",
        "    \n",
        "    text_features = list(convert_lst_to_features(\n",
        "        texts, SEQ_LEN, SEQ_LEN, \n",
        "        bert_tokenizer, log, False, False))\n",
        "\n",
        "    target_shape = (len(texts), -1)\n",
        "\n",
        "    feed_dict = {}\n",
        "    for iname in INPUT_NAMES:\n",
        "        features_i = np.array([getattr(f, iname) for f in text_features])\n",
        "        features_i = features_i.reshape(target_shape).astype(\"int32\")\n",
        "        feed_dict[iname] = features_i\n",
        "\n",
        "    return feed_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpSMbxKKLYVe",
        "colab_type": "text"
      },
      "source": [
        "tf.Estimators have a fun feature which makes them re-build and re-initialize the whole computational graph at each call to the predict function. \n",
        "\n",
        "So, in order to avoid the overhead, to the predict function we will pass a generator, which will yield the features to the model in a never-ending loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gne1y7etLaEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_input_fn(container):\n",
        "    \n",
        "    def gen():\n",
        "        while True:\n",
        "          try:\n",
        "            yield build_feed_dict(container.get())\n",
        "          except:\n",
        "            yield build_feed_dict(container.get())\n",
        "\n",
        "    def input_fn():\n",
        "        return tf.data.Dataset.from_generator(\n",
        "            gen,\n",
        "            output_types={iname: tf.int32 for iname in INPUT_NAMES},\n",
        "            output_shapes={iname: (None, None) for iname in INPUT_NAMES})\n",
        "    return input_fn\n",
        "\n",
        "class DataContainer:\n",
        "  def __init__(self):\n",
        "    self._texts = None\n",
        "  \n",
        "  def set(self, texts):\n",
        "    if type(texts) is str:\n",
        "      texts = [texts]\n",
        "    self._texts = texts\n",
        "    \n",
        "  def get(self):\n",
        "    return self._texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG8MrQ4ILc-l",
        "colab_type": "text"
      },
      "source": [
        "**model_fn** contains the specification of the model. In our case, it is loaded from the pbtxt file we saved in the previous step. \n",
        "\n",
        "The features are mapped explicitly to the corresponding input nodes with input_map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8m5Uch7Lf9A",
        "colab_type": "code",
        "outputId": "0e96da74-5ddd-4c61-86d4-fde71e21e529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def model_fn(features, mode):\n",
        "    with tf.gfile.GFile(GRAPH_PATH, 'rb') as f:\n",
        "        graph_def = tf.GraphDef()\n",
        "        graph_def.ParseFromString(f.read())\n",
        "        \n",
        "    output = tf.import_graph_def(graph_def,\n",
        "                                 input_map={k + ':0': features[k] for k in INPUT_NAMES},\n",
        "                                 return_elements=['final_encodes:0'])\n",
        "\n",
        "    return EstimatorSpec(mode=mode, predictions={'output': output[0]})\n",
        "  \n",
        "estimator = Estimator(model_fn=model_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using temporary folder as model directory: /tmp/tmpqgabu32k\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTEqJGX5LpXH",
        "colab_type": "text"
      },
      "source": [
        "Now we have everything we need to perform inference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYv18IqcLnQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch(iterable, n=1):\n",
        "    l = len(iterable)\n",
        "    for ndx in range(0, l, n):\n",
        "        yield iterable[ndx:min(ndx + n, l)]\n",
        "\n",
        "def build_vectorizer(_estimator, _input_fn_builder, batch_size=128):\n",
        "  container = DataContainer()\n",
        "  predict_fn = _estimator.predict(_input_fn_builder(container), yield_single_examples=False)\n",
        "  \n",
        "  def vectorize(text, verbose=False):\n",
        "    x = []\n",
        "    bar = Progbar(len(text))\n",
        "    for text_batch in batch(text, batch_size):\n",
        "      container.set(text_batch)\n",
        "      x.append(next(predict_fn)['output'])\n",
        "      if verbose:\n",
        "        bar.add(len(text_batch))\n",
        "      \n",
        "    r = np.vstack(x)\n",
        "    return r\n",
        "  \n",
        "  return vectorize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw33e3SRLnKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_vectorizer = build_vectorizer(estimator, build_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAAXDUrsLnGW",
        "colab_type": "code",
        "outputId": "49bfec0a-6e6f-4b98-d6c1-3a34bfa29e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "bert_vectorizer(64*['sample text']).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_YuBehNMKvT",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: exploring vector space with Projector\n",
        "\n",
        "*A* standalone version of BERT feature extractor is available in the [repository](https://github.com/gaphex/bert_experimental).\n",
        "\n",
        "Using the vectorizer we will generate embeddings for articles from the CORD-19 benchmark (in this tutorial, the Reuters-21578 benchmark corpus was used previously)\n",
        "\n",
        "To visualise and explore the embedding vector space in 3D we will use a dimensionality reduction technique called [T-SNE](https://distill.pub/2016/misread-tsne/).\n",
        "\n",
        "Lets get the article embeddings first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrWW7qxLLm9Z",
        "colab_type": "code",
        "outputId": "d69683d3-1e45-4c37-c25d-6212b98b5bf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "nltk.download(\"reuters\")\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jibjLu9Tx6i",
        "colab_type": "code",
        "outputId": "a92f1fe3-b74b-4510-ca5f-0bbd0d14d0d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(reuters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.corpus.reader.plaintext.CategorizedPlaintextCorpusReader"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pSFM8TOM1wU",
        "colab_type": "code",
        "outputId": "2d03fb15-6828-4670-8e7e-b067b1c6621c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# REUTERS EXAMPLE\n",
        "max_samples = 256\n",
        "categories = ['wheat', 'tea', 'strategic-metal', \n",
        "              'housing', 'money-supply', 'fuel']\n",
        "\n",
        "S, X, Y = [], [], []\n",
        "\n",
        "for category in categories:\n",
        "  print(category)\n",
        "  \n",
        "  sents = reuters.sents(categories=category)\n",
        "  sents = [' '.join(sent) for sent in sents][:max_samples]\n",
        "  X.append(bert_vectorizer(sents, verbose=True))\n",
        "  Y += [category] * len(sents)\n",
        "  S += sents\n",
        "  \n",
        "X = np.vstack(X) \n",
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wheat\n",
            "256/256 [==============================] - 5s 20ms/step\n",
            "tea\n",
            "154/154 [==============================] - 3s 20ms/step\n",
            "strategic-metal\n",
            "200/200 [==============================] - 4s 19ms/step\n",
            "housing\n",
            "139/139 [==============================] - 3s 20ms/step\n",
            "money-supply\n",
            "256/256 [==============================] - 5s 20ms/step\n",
            "fuel\n",
            "129/129 [==============================] - 3s 20ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1134, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSAPjrWkNPZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"embeddings.tsv\", \"w\") as fo:\n",
        "  for x in X.astype('float16'):\n",
        "    line = \"\\t\".join([str(v) for v in x])\n",
        "    fo.write(line + \"\\n\")\n",
        "\n",
        "with open(\"metadata.tsv\", \"w\") as fo:\n",
        "  fo.write(\"Label\\tSentence\\n\")\n",
        "  for y, s in zip(Y, S):\n",
        "    fo.write(\"{}\\t{}\\n\".format(y, s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqVYk769NiiB",
        "colab_type": "text"
      },
      "source": [
        "The interactive visualization of generated embeddings is available on the [Embedding Projector](https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/gaphex/7262af1e151957b1e7c638f4922dfe57/raw/3b946229fc58cbefbca2a642502cf51d4f8e81c5/reuters_proj_config.json). **<--CLICK THAT TO GENERATE**\n",
        "\n",
        "From the link you can run T-SNE yourself, or load a checkpoint using the bookmark in lower-right corner (loading works only on Chrome).\n",
        "\n",
        "To reproduce the input files used for this visualization, run the code below. Then, download the files to your machine and upload to Projector\n",
        "\n",
        "(you can dowload files from the menu opened by the \">\" button in the upper-left)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvqeITuSNjPk",
        "colab_type": "code",
        "outputId": "58bc4de4-b815-419c-a297-a5c08dca6c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        }
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"900\" height=\"632\" controls>\n",
        "  <source src=\"https://storage.googleapis.com/bert_resourses/reuters_tsne_hd.mp4\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"900\" height=\"632\" controls>\n",
              "  <source src=\"https://storage.googleapis.com/bert_resourses/reuters_tsne_hd.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSYC8R9zUH3E",
        "colab_type": "text"
      },
      "source": [
        "### Create embeddings for CORD19 Articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNknqrSIUL5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert pandas dataframe to nltk.corpus.reader.plaintext.CategorizedPlaintextCorpusReader\n",
        "# From: https://stackoverflow.com/questions/49088978/how-to-create-corpus-from-pandas-data-frame-to-operate-with-nltk/49104725\n",
        "def CreateCorpusFromDataFrame(corpusfolder,df):\n",
        "    for index, r in df.iterrows():\n",
        "        id=index\n",
        "        title=r['title']\n",
        "        body=r['title']\n",
        "        # handler text for not properly munged data\n",
        "        try: \n",
        "          category=re.sub('/', '', r['journal']) # remove odd characters as writing to file\n",
        "        except TypeError:\n",
        "          continue\n",
        "        fname=str(category)+'_'+str(id)+'.txt'\n",
        "        corpusfile=open(corpusfolder+'/'+fname,'a+')\n",
        "        corpusfile.write(str(body) +\" \" +str(title))\n",
        "        corpusfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmAcjhVqV2kK",
        "colab_type": "code",
        "outputId": "391a340e-2cd1-4d16-e99a-d501fdf86c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# create folder to hold CORD19 nltk\n",
        "dirName = 'CORD19_nltk_title_only'\n",
        "try:\n",
        "    # Create target Directory\n",
        "    os.mkdir(dirName)\n",
        "except FileExistsError:\n",
        "    pass\n",
        "\n",
        "# create corpus\n",
        "CreateCorpusFromDataFrame(dirName,metadata)\n",
        "print(\"Corpus created in folder: %s\" % dirName)"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus created in folder: CORD19_nltk_title_only\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yds9N-ggY4M2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the corpus reader\n",
        "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
        "\n",
        "# Create NLTK data structure (with pattern matching to create the article names again)\n",
        "CORD_corpus=CategorizedPlaintextCorpusReader(dirName,r'.*', cat_pattern=r'(.*)_.*.txt$') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kGn261zN8fZ",
        "colab_type": "code",
        "outputId": "dcfcb04d-9975-4ee2-a24a-e8f868bf95b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# total journals\n",
        "print(\"Total number journals: %s\" % (len(metadata.journal.unique())))\n",
        "\n",
        "# select a subset of journals, where the journal will be the tag\n",
        "num_journals=8\n",
        "categories=metadata['journal'].value_counts()[:num_journals].index.tolist()\n",
        "print (\"\\nPicking most common journals:\")\n",
        "categories\n",
        "\n"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number journals: 3929\n",
            "\n",
            "Picking most common journals:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PLoS One',\n",
              " 'Virology',\n",
              " 'Emerg Infect Dis',\n",
              " 'Viruses',\n",
              " 'The Lancet',\n",
              " 'Sci Rep',\n",
              " 'Virus Research',\n",
              " 'Vaccine']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0jyk0_9LmmO",
        "colab_type": "code",
        "outputId": "23ef4e54-f587-43c1-bc04-472c727d90ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "#CORD19 Examples\n",
        "max_samples = 5000\n",
        "\n",
        "S, X, Y = [], [], []\n",
        "\n",
        "for category in categories:\n",
        "  print(category)\n",
        "  \n",
        "  sents = CORD_corpus.sents(categories=category)\n",
        "  sents = [' '.join(sent) for sent in sents][:max_samples]\n",
        "  X.append(bert_vectorizer(sents, verbose=True))\n",
        "  Y += [category] * len(sents)\n",
        "  S += sents\n",
        "  \n",
        "X = np.vstack(X) \n",
        "X.shape"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PLoS One\n",
            "1645/1645 [==============================] - 34s 20ms/step\n",
            "Virology\n",
            "856/856 [==============================] - 18s 20ms/step\n",
            "Emerg Infect Dis\n",
            "811/811 [==============================] - 16s 20ms/step\n",
            "Viruses\n",
            "604/604 [==============================] - 12s 20ms/step\n",
            "The Lancet\n",
            "741/741 [==============================] - 15s 20ms/step\n",
            "Sci Rep\n",
            "493/493 [==============================] - 10s 21ms/step\n",
            "Virus Research\n",
            "513/513 [==============================] - 11s 21ms/step\n",
            "Vaccine\n",
            "534/534 [==============================] - 11s 21ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6197, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0M5K6rEnt-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make folder in google drive to download files\n",
        "location = '/content/drive/My Drive/'\n",
        "\n",
        "with open(location + \"embeddings_large.tsv\", \"w\") as fo:\n",
        "  for x in X.astype('float16'):\n",
        "    line = \"\\t\".join([str(v) for v in x])\n",
        "    fo.write(line + \"\\n\")\n",
        "\n",
        "with open(location + \"metadata_large.tsv\", \"w\") as fo:\n",
        "  fo.write(\"Label\\tSentence\\n\")\n",
        "  for y, s in zip(Y, S):\n",
        "    fo.write(\"{}\\t{}\\n\".format(y, s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3JbwudUnzrU",
        "colab_type": "text"
      },
      "source": [
        "The interactive visualization of generated embeddings is available on the [Embedding Projector](https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/gaphex/7262af1e151957b1e7c638f4922dfe57/raw/3b946229fc58cbefbca2a642502cf51d4f8e81c5/reuters_proj_config.json). **<--CLICK THAT TO GENERATE**\n",
        "\n",
        "Then go to bottom right and load in those files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jLduttHn2P1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}